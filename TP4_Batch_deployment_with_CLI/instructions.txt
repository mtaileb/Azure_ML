git clone https://github.com/Azure/azureml-examples --depth 1
cd azureml-examples/cli
cd endpoints/batch/deploy-models/mnist-classifier

# Create compute cluster:
# Batch endpoints run on compute clusters. They support both Azure Machine Learning
# Compute clusters (AmlCompute) or Kubernetes clusters. Clusters are a shared resource
# so one cluster can host one or many batch deployments (along with other workloads if desired).
az ml compute create -n batch-cluster --type amlcompute --min-instances 0 --max-instances 2


# Create a batch endpoint:
# A batch endpoint is an HTTPS endpoint that clients can call to trigger a batch scoring job
ENDPOINT_NAME="mnist-batch"

# Copy endpoint.yaml (which defines a batch endpoint, which you can include in the CLI command for batch endpoint creation) in local directory:
wget https://github.com/mtaileb/Azure_ML/raw/main/TP4_Batch_deployment_with_CLI/endpoint.yml

# Create endpoint:
az ml batch-endpoint create --file endpoint.yml  --name $ENDPOINT_NAME
MODEL_NAME='mnist-classifier-torch'
az ml model create --name $MODEL_NAME --type "custom_model" --path "deployment-torch/model"

# Copy the scoring script patch_driver.py. Batch deployments require a scoring script that indicates how a
# given model should be executed and how input data must be processed. Batch Endpoints 
# support scripts created in Python. In this case, we're deploying a model that reads image 
# files representing digits and outputs the corresponding digit. The scoring script is 
wget https://github.com/mtaileb/Azure_ML/raw/main/TP4_Batch_deployment_with_CLI/batch_driver.py deployment-torch/code

# Create an environment where your batch deployment will run. In this case, the
# dependencies have been captured in a conda.yaml:
wget https://github.com/mtaileb/Azure_ML/raw/main/TP4_Batch_deployment_with_CLI/conda.yaml deployment-torch/environment

# Create environment from this file:
wget https://github.com/mtaileb/Azure_ML/raw/main/TP4_Batch_deployment_with_CLI/env.yaml

# Create deployment definition:
wget https://github.com/mtaileb/Azure_ML/raw/main/TP4_Batch_deployment_with_CLI/deployment.yml deployment-torch

# Create deployment:
az ml batch-deployment create --file deployment-torch/deployment.yml --endpoint-name $ENDPOINT_NAME --set-default

# Check batch endpoint and deployment details:
DEPLOYMENT_NAME="mnist-torch-dpl"
az ml batch-deployment show --name $DEPLOYMENT_NAME --endpoint-name $ENDPOINT_NAME


# Run batch endpoints and access results
# Invoking a batch endpoint triggers a batch scoring job. The job name will be returned 
# from the invoke response and can be used to track the batch scoring progress. When 
# running models for scoring in Batch Endpoints, you need to indicate the input data path 
# where the endpoints should look for the data you want to score. The following example 
# shows how to start a new job over a sample data of the MNIST dataset stored in an Azure 
# Storage Account.
JOB_NAME=$(az ml batch-endpoint invoke --name $ENDPOINT_NAME --input https://azuremlexampledata.blob.core.windows.net/data/mnist/sample --input-type uri_folder --query name -o tsv)

# Monitor batch job execution progress
# Batch scoring jobs usually take some time to process the entire set of inputs:
az ml job show -n $JOB_NAME --web

# Check batch scoring results
# The job outputs will be stored in cloud storage, either in the workspace's default blob storage, or the storage you specified.
# Run the following code to open batch scoring job in Azure Machine Learning studio. The 
# job studio link is also included in the response of invoke, as the value of 
# interactionEndpoints.Studio.endpoint.
az ml job show -n $JOB_NAME --web
# In the graph of the job, select the batchscoring step.
# Select the Outputs + logs tab and then select Show data outputs.
# From Data outputs, select the icon to open Storage Explorer.
